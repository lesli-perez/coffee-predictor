# -*- coding: utf-8 -*-
"""ZhangProject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZUd9mjcbSX2vhoWwj7NCL0D-64gvvpQC

**Data Mining 6366 Project**

**Team Members:** <br>
Lesli Perez<br>
Jennifer Pedraza<br>
Rebecca Fernandez<br>
Ashley Gomez
"""

# importing libraries
import random
import pandas as pd
from sklearn.model_selection import train_test_split
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from collections import Counter
import matplotlib.pyplot as plt
from sklearn.metrics import f1_score
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
nltk.download('punkt')
from nltk.sentiment.vader import SentimentIntensityAnalyzer
nltk.download('wordnet')
nltk.download('vader_lexicon')


# set seed for reproducibility
random.seed(6366)

# read all given csv files
x_db = pd.read_csv("/content/X_test.csv")
x_train = pd.read_csv("/content/X_train.csv")
y_train = pd.read_csv("/content/y_train.csv")

roaster = x_train["roaster"]
classes = y_train["class"]

class_0_roasters = roaster[classes == 0]
class_1_roasters = roaster[classes == 1]

class_0_roaster_counts = class_0_roasters.value_counts().head(20)
class_1_roaster_counts = class_1_roasters.value_counts().head(20)

# Plotting
plt.bar(class_0_roaster_counts.index, class_0_roaster_counts.values, color='lightblue', label='Class 0')
plt.bar(class_1_roaster_counts.index, class_1_roaster_counts.values, color='hotpink', label='Class 1')

plt.xlabel('Roaster')
plt.ylabel('Frequency')
plt.title('Top Roasters in Class 0 and 1')
plt.xticks(rotation=90)
plt.legend()

plt.show()

#Making the roaster column into a categorical to integer using pandas
print(x_train.head())
x_traindum = pd.get_dummies(x_train, columns=['roaster'])

# print(x_traindum.head())
# print(x_testrrdum.head())
print(x_traindum.shape,'Roaster')

#Making the roast into dummies
x_traindumroast=pd.get_dummies(x_traindum, columns=['roast'])


# print(x_traindumroast.head())
# print(x_testrrdumroast.head())
print(x_traindumroast.shape,'Roast')


#Making the orgin into dummies
x_traindumroastorg=pd.get_dummies(x_traindumroast, columns=['origin'])

x_train = x_traindumroastorg

print(x_traindumroastorg.shape)
print(x_train.shape)

# print(x_traindumroastorg.head())
# print(x_testrrdumroastorg.head())

print(x_traindumroastorg.shape,'Origin')

# split training data 80/20 for validation <3
# before splitting, join y_train to x_train as an extra column
# (making sure the y train rows are deleted along with null x train for consistency, since there is no ID in y_train)
# split 80/20 then make y_train and y_test by removing the last column

# merge x and y train
training_data = pd.concat([x_train, y_train], axis=1)

y_train = training_data.iloc[:, -1]
x_train = training_data.iloc[:, :-1]

# split data
train_data, test_data = train_test_split(training_data, test_size=0.1, random_state=42)


# unmerge x and y train and x and y test
y_train = train_data.iloc[:, -1]
x_train = train_data.iloc[:, :-1]

y_test = test_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
print(x_test.shape)
print(y_test.shape)

reviews = training_data["review"]
sentiments = training_data["class"]

neg_rev = []
pos_rev = []

for review, sentiment in zip(reviews, sentiments):
    if sentiment == 0:
        neg_rev.append({"review": review})
    else:
        pos_rev.append({"review": review})

# Convert to DataFrame
neg_df = pd.DataFrame(neg_rev)
pos_df = pd.DataFrame(pos_rev)

len(neg_rev)

# Remove punctuation and get word frequency from negative reviews
words = []
for entry in neg_rev:
    review = entry["review"]
    # Split the review into words
    review_words = review.split()
    # Remove punctuation
    for char in ",.;()'":
        review_words = [word.replace(char, "") for word in review_words]
    # Convert words to lowercase and add to the list
    words.extend([word.lower() for word in review_words])

# Get word frequency
word_frequency_neg = Counter(words)

# Add custom stopwords
stopwords = set(stopwords.words('english'))
custom_stopwords = ["cup", "the", "cup.", "notes", "structure", "finish", "finish.", "very", "long", "resonant", "hint", "long,", "finish", "on.", "go"]
stopwords.update(custom_stopwords)

# Remove stopwords from word frequency
for stopword in stopwords:
    if stopword in word_frequency_neg:
        del word_frequency_neg[stopword]

# Print the most common words
print(word_frequency_neg.most_common(20))

# Remove punctuation and get word frequency from positive reviews
words = []
for entry in pos_rev:
    review = entry["review"]
    # Split the review into words
    review_words = review.split()
    # Remove punctuation
    for char in ",.;()'":
        review_words = [word.replace(char, "") for word in review_words]
    # Convert words to lowercase and add to the list
    words.extend([word.lower() for word in review_words])

# Get word frequency
word_frequency_pos = Counter(words)

# Remove stopwords from word frequency
for stopword in stopwords:
    if stopword in word_frequency_pos:
        del word_frequency_pos[stopword]

# Print the most common words
print(word_frequency_pos.most_common(20))

negative_words = []
for word in word_frequency_neg:
  if word not in word_frequency_pos:
    negative_words.append(word)
print(negative_words)

positive_words = []
for word in word_frequency_pos:
  if word not in word_frequency_neg:
    positive_words.append(word)
print(positive_words)

all_words = []

for entry in pos_rev:
    review = entry["review"]
    review_words = review.split()

    # Remove punctuation
    for char in ",.;()'":
        review_words = [word.replace(char, "") for word in review_words]

    # Convert words to lowercase and add to the list
    all_words.extend([word.lower() for word in review_words])


word_frequency = Counter(all_words)

# Remove stopwords from word frequency
for stopword in stopwords:
    if stopword in word_frequency:
        del word_frequency[stopword]

# Print the most common words
print(word_frequency.most_common(20))

most_common_words = word_frequency.most_common(20)

words, frequencies = zip(*most_common_words)

plt.figure(figsize=(10, 6))
plt.barh(words, frequencies, color = "hotpink")
plt.xlabel('Frequency')
plt.ylabel('Words')
plt.title('Most Common Words in Reviews')
plt.gca().invert_yaxis()  # most frequent words on top
plt.show()

top_negative_words = negative_words[:20]
top_negative_frequencies = [word_frequency_neg[word] for word in top_negative_words]

plt.figure(figsize=(10, 6))
plt.barh(top_negative_words, top_negative_frequencies, color="hotpink")
plt.xlabel('Frequency')
plt.ylabel('Negative Words')
plt.title('Top 20 Most Common Negative Words in Reviews')
plt.gca().invert_yaxis()  # higher frequency words on top
plt.show()

from wordcloud import WordCloud
wordcloud_dict = {word: freq for word, freq in zip(top_negative_words, top_negative_frequencies)}

wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(wordcloud_dict)

plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud of Top 20 Most Common Negative Words in Reviews')
plt.show()

top_positive_words = positive_words[:20]
top_positive_frequencies = [word_frequency_pos[word] for word in top_positive_words]

plt.figure(figsize=(10, 6))
plt.barh(top_positive_words, top_positive_frequencies, color="hotpink")
plt.xlabel('Frequency')
plt.ylabel('Positive Words')
plt.title('Top 20 Most Common Positive Words in Reviews')
plt.gca().invert_yaxis()  # higher frequency words on top
plt.show()

from wordcloud import WordCloud
wordcloud_dict2 = {word: freq for word, freq in zip(top_positive_words, top_positive_frequencies)}

wordcloud2 = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(wordcloud_dict2)

plt.figure(figsize=(10, 6))
plt.imshow(wordcloud2, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud of Top 20 Most Common Positive Words in Reviews')
plt.show()

# initialize NLTK sentiment analyzer
analyzer = SentimentIntensityAnalyzer()

# create get_sentiment function
def get_sentiment(text):
    scores = analyzer.polarity_scores(text)
    # Check if any negative words are present in the text
    negative_present = any(word in text.lower() for word in negative_words)
    if scores['pos'] > 0 and not negative_present:
        sentiment = 1  # Positive sentiment if positive score and no negative words
    else:
        sentiment = 0  # Negative sentiment otherwise
    return sentiment

print(x_train.size)

# apply get_sentiment function
x_train['sentiment'] = x_train['review'].apply(get_sentiment)
count = 0
for i, sentiment in enumerate(x_train["sentiment"]):
    if sentiment == 0:
        # print(x_train.iloc[i])
        count += 1

print("Total instances with sentiment 0:", count)

x_train.head()

# apply get_sentiment function
x_test['sentiment'] = x_test['review'].apply(get_sentiment)
count = 0
for i, sentiment in enumerate(x_test["sentiment"]):
    if sentiment == 0:
        # print(x_train.iloc[i])
        count += 1

print("Total instances with sentiment 0:", count)

x_test.head()
print(x_test.shape)

print(x_train.columns)
# Printing the 'review' column from rows with sentiment column equal to 1
print('These are the Outstanding notes ',(x_train.loc[x_train['sentiment'] == 1, 'review']).head(10))

# Printing the 'review' column from rows with sentiment column equal to 0
print('These are the Average notes ' ,(x_train.loc[x_train['sentiment'] == 0, 'review']).head(10))

print(x_train.shape)

n = x_train.nunique(axis=0)

print("Number of unique values in each column :\n",
      n)

x_train.columns

#https://www.kaggle.com/code/prashant111/naive-bayes-classifier-in-python#4.-Applications-of-Naive-Bayes-algorithm-
categorical = [var for var in x_train.columns if x_train[var].dtype=='O']

print('There are {} categorical variables\n'.format(len(categorical)))

print('The categorical variables are :\n\n', categorical)

"""F1 Measurment is 2a/2a+b+c  Aka 2(TP)/ 2(TP)+FP+FN"""

x_train = x_train.drop(columns=["review"])
x_train = x_train.drop(columns=["coffee_id"])
x_test = x_test.drop(columns=["review"])
x_test = x_test.drop(columns=["coffee_id"])

from sklearn.naive_bayes import GaussianNB

# Build a Gaussian Classifier
model = GaussianNB()
# Train the model using the training sets
# Model training
model.fit(x_train, y_train)

# Predict Output
predicted = model.predict([x_test.iloc[5]])
print(predicted)
print("Actual Value:", y_test.iloc[5])
print("Predicted Value:", predicted[0])

from sklearn.metrics import (
    accuracy_score,
    confusion_matrix,
    ConfusionMatrixDisplay,
    f1_score,
)

# print(x_test.shape)
# print(y_test.shape)

# print(y_test.head())
# print(x_test.head())

y_pred = model.predict(x_test)

# print(y_pred.size)
# print(y_test.size)
accuray = accuracy_score(y_pred, y_test)
f1 = f1_score(y_pred, y_test, average="weighted")

print("Accuracy for Model 1:", accuray)
print("F1 Score for Model 1:", f1)

print()

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
pink_cmap = ListedColormap(['#FFE1E6', '#FFC7D1', '#FFADC0', '#FF93B0', '#FF79A1', '#FF5F91', '#FF467F'])


# A matrix that can show true postitives and false negatives
labels = ["Average", "Outstanding"]
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)
disp.plot(cmap=pink_cmap);
plt.show()

#Making the roaster column into a categorical to integer using pandas
print(x_db.head())
x_testdum = pd.get_dummies(x_db, columns=['roaster'])

# print(x_traindum.head())
# print(x_testrrdum.head())
print(x_testdum.shape,'Roaster')

#Making the roast into dummies
x_testdumroast=pd.get_dummies(x_testdum, columns=['roast'])


# print(x_traindumroast.head())
# print(x_testrrdumroast.head())
print(x_testdumroast.shape,'Roast')


#Making the orgin into dummies
x_testdumroastorg=pd.get_dummies(x_traindumroast, columns=['origin'])

x_db = x_testdumroastorg

print(x_testdumroastorg.shape)
print(x_db.shape)

# print(x_traindumroastorg.head())
# print(x_testrrdumroastorg.head())

print(x_testdumroastorg.shape,'Origin')

# apply get_sentiment function
x_db['sentiment'] = x_db['review'].apply(get_sentiment)
count = 0
for i, sentiment in enumerate(x_db["sentiment"]):
    if sentiment == 0:
        # print(x_train.iloc[i])
        count += 1

print("Total instances with sentiment 0:", count)

x_db.head()
print(x_db.shape)
x_db = x_db.drop(columns=['review'])
x_db = x_db.drop(columns=['coffee_id'])

# DO MODEL ON TESTING DATA
y_pred_test = model.predict(x_db)

# PRINT AS CSV
df = pd.DataFrame({'class': y_pred_test})
filename = "result.csv"
df.to_csv(filename, index=False)

pos = 0
neg = 0

for x in y_pred_test:
  if x == 0:
    neg += 1
  else:
    pos += 1

print("Total predictions:", len(y_pred_test))
print("Total positive:", pos)
print("Total negative:", neg)